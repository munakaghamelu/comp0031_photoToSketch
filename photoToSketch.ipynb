{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "photoToSketch.ipynb",
      "provenance": [],
      "mount_file_id": "16WI5tqzicbwJ7UWeTeQoNr_vWrVwfZII",
      "authorship_tag": "ABX9TyP7JTf0U5izIhZ8GKVI6GZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munakaghamelu/comp0031_photoToSketch/blob/main/photoToSketch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS5XMHlyeFYa"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import random\n",
        "random.seed(0)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHBhLEMWjF8L"
      },
      "source": [
        "image_size = 100\n",
        "\n",
        "class AutoEncoder(Module):\n",
        "    # IMPLEMENT THIS AUTO-ENCODER CLASS\n",
        "    def __init__(self,bottleneck_size):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        # INITIALIZE YOUR TRAINING PARAMETERS HERE.\n",
        "        input_shape = image_size**2*3\n",
        "        self.enc_h1 = nn.Linear(in_features=input_shape, out_features=50)\n",
        "        self.enc_out = nn.Linear(in_features=50, out_features=bottleneck_size)\n",
        "\n",
        "        self.enc_mu = nn.Linear(2*50, Nz)\n",
        "        self.enc_sigma = nn.Linear(2*50, Nz)\n",
        "        \n",
        "        self.dec_h1 = nn.Linear(in_features=Nz, 2*dec_hidden_size)\n",
        "        self.lstm = nn.LSTM(Nz+5, dec_hidden_size, dropout=dropout)\n",
        "        self.dec_params = nn.Linear(dec_hidden_size, 6*M+3)\n",
        "\n",
        "        # self.dec_h1 = nn.Linear(in_features=bottleneck_size, out_features=50)\n",
        "        # self.dec_out = nn.Linear(in_features=50, out_features=input_shape)\n",
        "        \n",
        "    def encoder(self,image):\n",
        "        # WRITE ENCODER ARCHITECTURE HERE\n",
        "        h1_ac = torch.relu(self.enc_h1(image))\n",
        "        mu = self.enc_mu(h1_ac)\n",
        "        logVar = self.sigma(h1_ac)\n",
        "        std = logVar.mul(0.5).exp()\n",
        "        z_size = mu.size()\n",
        "        N = torch.normal(torch.zeros(z_size), torch.ones(z_size))\n",
        "        z = mu + std*N\n",
        "        # mu and logVar are need for LKL, this is because its a variational autoencoder model\n",
        "        return z, mu, logVar\n",
        "    \n",
        "    def decoder(self, concatenated_vector, z, hidden_cell=None):\n",
        "        # WRITING DECODER FOR SKETCH-RNN\n",
        "        if hidden_cell is None:\n",
        "          hidden, cell = torch.split(F.tanh(self.dec_h1(z)),dec_hidden_size,1)\n",
        "          hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
        "          outputs,(hidden, cell) = self.lstm(concatenated_vector, hidden_cell)\n",
        "        if self.training:\n",
        "            y = self.dec_params(outputs.view(-1, dec_hidden_size))\n",
        "        else:\n",
        "            y = self.dec_params(hidden.view(-1, dec_hidden_size))\n",
        "        # separate pen and mixture params:\n",
        "        params = torch.split(y,6,1)\n",
        "        params_mixture = torch.stack(params[:-1]) # trajectory\n",
        "        params_pen = params[-1] #pen up/down\n",
        "        # identify mixture params:\n",
        "        pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy = torch.split(params_mixture,1,2)\n",
        "        # preprocess params:\n",
        "        if self.training:\n",
        "            len_out = Nmax+1\n",
        "        else:\n",
        "            len_out = 1\n",
        "                                   \n",
        "        pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,M)\n",
        "        sigma_x = torch.exp(sigma_x.transpose(0,1).squeeze()).view(len_out,-1,M)\n",
        "        sigma_y = torch.exp(sigma_y.transpose(0,1).squeeze()).view(len_out,-1,M)\n",
        "        rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(len_out,-1,M)\n",
        "        mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(len_out,-1,M)\n",
        "        mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(len_out,-1,M)\n",
        "        q = F.softmax(params_pen).view(len_out,-1,3)\n",
        "        return pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,hidden,cell\n",
        "    \n",
        "    def forward(self,image):\n",
        "        # PUT IT TOGETHER HERE\n",
        "        # Get the inputs ready for the decoder part\n",
        "        #batch, lengths = make_batch(batch_size)\n",
        "        z, mu, sigma = self.encoder(image)\n",
        "        # could work on changing the position 1 is placed\n",
        "        sos = torch.stack([torch.Tensor([0,0,1,0,0])]*batch_size).unsqueeze(0)\n",
        "        batch_init = torch.cat([sos, sos],0)\n",
        "        z_stack = torch.stack([z]*(Nmax+1))\n",
        "        inputs = torch.cat([batch_init, z_stack],2)\n",
        "        self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy, self.q, _, _ = self.decoder(inputs, z)\n",
        "        #decoded_image = self.decoder(code)\n",
        "        return decoded_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8FGlG-tgSIj"
      },
      "source": [
        "class Chairs(Dataset):\n",
        "    # IMPLEMENT THIS DATA LOADING CLASS\n",
        "    def __init__(self, dataset_path=\"\"):\n",
        "        # DEFINE YOUR PARAMETERS AND VARIABLES YOU NEED HERE.\n",
        "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])\n",
        "        self.input_imgs_names = sorted(os.listdir(dataset_path))\n",
        "        self.input_imgs_paths = [os.path.join(dataset_path, name) for name in self.input_imgs_names]\n",
        "        \n",
        "    def __len__(self):\n",
        "        # RETURN SIZE OF DATASET\n",
        "        length = len(self.input_imgs_names)\n",
        "        return length\n",
        "\n",
        "    # cv2 transforms data into tensors\n",
        "    def __getitem__(self, idx):\n",
        "        # RETURN IMAGE AT GIVEN idx\n",
        "        # use cv2 to load images\n",
        "        img = cv2.imread(self.input_imgs_paths[idx])\n",
        "        input_image = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "        input_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        input_image = cv2.resize(input_image, (image_size, image_size), interpolation=cv2.INTER_NEAREST)\n",
        "        input_image = self.transform(input_image)\n",
        "        return input_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPU7hBQpgZV7"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "dec_image_size = 512\n",
        "Nz = 128\n",
        "M = 20\n",
        "batch_size = 20 #encoder code uses 100\n",
        "dropout = 0.9\n",
        "\n",
        "chairs_train_data_path = \"/content/drive/MyDrive/sketch-rnn/Datasets/quickdraw/chair/train\"\n",
        "chairs_train_dataset = Chairs(chairs_train_data_path)\n",
        "torch_train_chairs = DataLoader(chairs_train_dataset, shuffle=True, batch_size=batch_size, num_workers=1)\n",
        "\n",
        "chairs_val_data_path = \"/content/drive/MyDrive/sketch-rnn/Datasets/quickdraw/chair/val\"\n",
        "chairs_val_dataset = Chairs(chairs_val_data_path)\n",
        "torch_val_chairs = DataLoader(chairs_val_dataset, shuffle=True,batch_size=batch_size,num_workers=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POrdjNrw-o7s"
      },
      "source": [
        "def reconstruction_loss(producedSketch, desiredSketch):\n",
        "    # L2 RECONSTRUCTION LOSS\n",
        "    criterion = nn.MSELoss()\n",
        "    return criterion(producedSketch, desiredSketch)\n",
        "\n",
        "epochs = 100 # CHOOSE YOUR EPOCH SIZE TO GET BEST RESULTS\n",
        "\n",
        "# May not need this if using pytorch model\n",
        "chairs_bottleneck_size = 25 # CHOOSE YOUR BOTTLENECK SIZE. \n",
        "\n",
        "model_chairs = AutoEncoder(chairs_bottleneck_size)\n",
        "\n",
        "# Optimizer section, using Adam\n",
        "chairs_optimizer = optim.Adam(model_chairs.parameters(), lr=1e-3) \n",
        "\n",
        "train_loss = []\n",
        "for ep in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i,data in enumerate(torch_train_chairs):\n",
        "        # This is dependent on your pytorch model specification\n",
        "\n",
        "        # reshape mini-batch data to [N, 784] matrix so it can be loaded\n",
        "        data = data.view(-1, data.shape[1]*data.shape[2]*data.shape[3])\n",
        "        \n",
        "        # resetting the gradients back to zero, pytorch accumulates gradients each backward pass\n",
        "        chairs_optimizer.zero_grad()\n",
        "        \n",
        "        # encoder layer pass, compute the reconstructions\n",
        "        new_img = model_chairs(data)\n",
        "        \n",
        "        # compute loss to optimize reconstruction with\n",
        "        loss = reconstruction_loss(new_img, data)\n",
        "        \n",
        "        # back propagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # updtae parameters\n",
        "        chairs_optimizer.step()\n",
        "        \n",
        "        # add the mini-batch training loss to epoch loss\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    loss = running_loss / 100 # 100 images\n",
        "    \n",
        "    #train_loss.append(loss)\n",
        "    print(f'Epoch {ep+1} of {epochs}, Train Loss: {loss:.5f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWd7bZWE_Dze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}